{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d411c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4a007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_longitudinal_data(df):\n",
    "\n",
    "    print(\"Processing dementia_dataset.csv (longitudinal)...\")\n",
    "    # This is an efficient O(N) filtering operation.\n",
    "    processed_df = df.loc[df['Visit'] == 1].copy()\n",
    "    \n",
    "    # Rename columns for consistency. This is a fast, constant-time operation.\n",
    "    processed_df.rename(columns={'EDUC': 'Education', 'MMSE': 'MMSE', 'CDR': 'CDR', 'M/F': 'Gender'}, inplace=True)\n",
    "    \n",
    "    # Create the target variable 'Dementia'. This O(N) mapping is efficient.\n",
    "    # 'Nondemented' = 0, 'Demented' or 'Converted' = 1\n",
    "    group_map = {'Nondemented': 0, 'Demented': 1, 'Converted': 1}\n",
    "    processed_df['Dementia'] = processed_df['Group'].map(group_map)\n",
    "    \n",
    "    # Standardize 'Gender' column (F=0, M=1). Another efficient O(N) map.\n",
    "    processed_df['Gender'] = processed_df['Gender'].map({'F': 0, 'M': 1})\n",
    "\n",
    "    # Select and return only the columns we need.\n",
    "    features = ['Age', 'Education', 'Gender', 'MMSE', 'CDR', 'Dementia']\n",
    "    print(f\"Found {len(processed_df)} first-visit subjects.\")\n",
    "    return processed_df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3144f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cross_sectional_data(df):\n",
    "    print(\"\\nProcessing oasis_cross-sectional.csv...\")\n",
    "    processed_df = df.copy()\n",
    "    \n",
    "    # Rename columns for consistency.\n",
    "    processed_df.rename(columns={'Educ': 'Education', 'MMSE': 'MMSE', 'CDR': 'CDR', 'M/F': 'Gender'}, inplace=True)\n",
    "    \n",
    "    # Create the target variable 'Dementia'. Vectorized np.where is very fast (O(N)).\n",
    "    # A Clinical Dementia Rating (CDR) greater than 0 indicates dementia.\n",
    "    processed_df['Dementia'] = np.where(processed_df['CDR'] > 0, 1, 0)\n",
    "    \n",
    "    # Standardize 'Gender' column (F=0, M=1).\n",
    "    processed_df['Gender'] = processed_df['Gender'].map({'F': 0, 'M': 1})\n",
    "    \n",
    "    # Select and return only the columns we need.\n",
    "    features = ['Age', 'Education', 'Gender', 'MMSE', 'CDR', 'Dementia']\n",
    "    print(f\"Found {len(processed_df)} subjects.\")\n",
    "    return processed_df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2949a0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the loading, processing, combining, and saving of datasets.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        long_df = pd.read_csv(\"dementia_dataset.csv\")\n",
    "        cross_df = pd.read_csv(\"oasis_cross-sectional.csv\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}. Make sure 'dementia_dataset.csv' and 'oasis_cross-sectional.csv' are in the same folder.\")\n",
    "        return\n",
    "\n",
    "    # Process each DataFrame using the dedicated functions.\n",
    "    long_df_clean = process_longitudinal_data(long_df)\n",
    "    cross_df_clean = process_cross_sectional_data(cross_df)\n",
    "    \n",
    "    # --- Step 3: Combine and Finalize ---\n",
    "    print(\"\\nCombining datasets...\")\n",
    "    # pd.concat is optimized for this operation.\n",
    "    combined_df = pd.concat([long_df_clean, cross_df_clean], ignore_index=True)\n",
    "\n",
    "    # Drop rows with any missing values in our key columns. O(N) operation.\n",
    "    initial_rows = len(combined_df)\n",
    "    combined_df.dropna(inplace=True)\n",
    "    print(f\"Dropped {initial_rows - len(combined_df)} rows with missing values.\")\n",
    "    \n",
    "    # Ensure all feature columns are numeric. This loop is efficient as it runs a fixed number of times.\n",
    "    feature_cols = ['Age', 'Education', 'Gender', 'MMSE', 'CDR']\n",
    "    for col in feature_cols:\n",
    "        combined_df[col] = pd.to_numeric(combined_df[col], errors='coerce')\n",
    "        \n",
    "    combined_df.dropna(inplace=True)\n",
    "\n",
    "    print(f\"\\nTotal combined and cleaned rows: {len(combined_df)}\")\n",
    "    \n",
    "    # Save the final dataset to a new CSV file.\n",
    "    output_filename = 'combined_dementia_dataset.csv'\n",
    "    combined_df.to_csv(output_filename, index=False)\n",
    "    print(f\"Successfully created '{output_filename}'. You can now proceed to the next step.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5306e2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mMain function to orchestrate the loading, processing, combining, and saving of datasets.\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     long_df = \u001b[43mpd\u001b[49m.read_csv(\u001b[33m\"\u001b[39m\u001b[33mdementia_dataset.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m     cross_df = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33moasis_cross-sectional.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
